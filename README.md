# Structured Evals

![main ci](https://github.com/binkjakub/structured-evals/actions/workflows/main.yml/badge.svg)
![versions](https://img.shields.io/badge/Python-3.12%20|%203.13-blue)

Evaluation framework for structured outputs generated by LLMs.

> [!WARNING]
> This library is still in development and should not be used in production, API may change frequently.

## Installation

```bash
pip install -i https://test.pypi.org/simple/ structured-evals
```

## Usage
The library allows to evaluate structured outputs generated by LLMs, like (nested) dictionaries or lists.
It allows to define expected structure and evaluate predictions against some defined gold outputs.

Assume that we asked LLM to extract information about `name`, `age` and `birthday from a given text, and it generated the following output:
```yaml
name: John Doe
age: 42
birthday: 1990-01-01
```

To perform evaluation we need to define structure of expected output and how it should be evaluated with the provided evaluators:
```python
from structured_evals import BatchDictEval, DateEval, DictEval, EvalTextualMetric, EvaluationBatch, NumEval
evaluator = BatchDictEval(
    item_evaluator=DictEval(
        eval_mapping={
            "name": EvalTextualMetric(chrf_eval, "chrf"),
            "age": NumEval(),
            "birthday": DateEval(),
        }
    ),
    aggregation="average",
    error_strategy="ignore",
)
```

### Available evaluators
| Name                | Description                                                          |
|---------------------|----------------------------------------------------------------------|
| `NumEval`           | compares equality of two numbers                                     |
| `DateEval`          | compares equality of two dates (with a given format)                 |
| `EvalTextualMetric` | compares two texts by a given callable metric function (e.g. `chrf`) |
| `DictEval`          | compares two dictionaries by applying evaluators to each key         |
| `BatchDictEval`     | compares two lists of dictionaries by applying DictEval to each item |


### Example
For more comprehensive use cases, see [examples](examples) directory. Here, we provide a quick start script, which you can play with:

```python
import datetime
from pprint import pprint

from torchmetrics.functional.text import chrf_score

from structured_evals import BatchDictEval, DateEval, DictEval, EvalTextualMetric, EvaluationBatch, NumEval

eval_batch = EvaluationBatch(
    pred=[
        {
            "name": "John Doe",
            "age": 42,
            "birthday": datetime.datetime(1990, 1, 1).date(),
            "occupation": "Engineer",
        }
    ],
    target=[{"name": "John Doe", "age": 42, "birthday": datetime.datetime(1991, 1, 2).date()}],
)


def chrf_eval(pred: str, target: str) -> float:
    return chrf_score([pred], [target], n_char_order=1, n_word_order=0).item()  # type: ignore


evaluator = BatchDictEval(
    item_evaluator=DictEval(
        eval_mapping={
            "name": EvalTextualMetric(chrf_eval, "chrf"),
            "age": NumEval(),
            "birthday": DateEval(),
        }
    ),
    aggregation="average",
    error_strategy="ignore",
)

results = evaluator(pred=eval_batch.pred, target=eval_batch.target)

pprint(results.agg_results)
```

## Citation

```bibtex
# TBA
```

## See also

* [JuDDGES - Judicial Decision Data Gathering, Encoding, and Sharing](https://github.com/pwr-ai/JuDDGES)
