# Structured Evals

![main ci](https://github.com/binkjakub/structured-evals/actions/workflows/main.yml/badge.svg)
![versions](https://img.shields.io/badge/Python-3.12%20|%203.13-blue)

Evaluation framework for structured outputs generated by LLMs.

> [!WARNING]
> This library is still in development and should not be used in production, API may change frequently.

## Installation


Clone the repository and install dependencies with development tools:

```bash
git clone https://github.com/binkjakub/structured-evals.git
cd structured-evals
make install
```

This will install all dependencies including development tools using `uv sync --locked --all-extras --dev` (see [Makefile](/Makefile)).

## Usage

The library provides a command-line interface for evaluating structured outputs generated by LLMs. The CLI automatically infers the appropriate evaluators and performs the evaluation.

### Quick Start

The CLI supports two main evaluation modes:

#### 1. Evaluation using a schema file

When you have a YAML schema file that describes the expected structure:

```bash
# Using uv (recommended)
uv run python -m structured_evals.cli eval-from-schema \
    predictions.json \
    schema.yaml \
    --output results.json

# Or if installed globally
structured-evals eval-from-schema predictions.json schema.yaml --output results.json
```

#### 2. Evaluation by inferring structure from data

When you want to automatically infer the evaluation structure from your target data:

```bash
# Using uv (recommended)
uv run python -m structured_evals.cli eval-from-predictions \
    predictions.json \
    --output results.json

# Or if installed globally
structured-evals eval-from-predictions predictions.json --output results.json
```

### CLI Options

Both commands support the following options:

- `--output`, `-o`: Output file for results (default: `results.json`)
- `--pred-key`: Key for predictions in JSON file (default: `answer`)
- `--target-key`: Key for targets in JSON file (default: `gold`)
- `--text-evaluator`: Text evaluator to use (default: `llm`)
- `--verbose`, `-v`: Enable verbose output

### Input Format

Your predictions file should be a JSON file with the following structure:
```json
[
  {
    "answer": "{\"name\": \"John Doe\", \"age\": 42}",
    "gold": "{\"name\": \"John Doe\", \"age\": 42}"
  }
]
```

### Schema Format

When using `eval-from-schema`, provide a YAML schema file describing the expected structure:
```yaml
name:
  type: string
  description: "Person's name"
  required: true

age:
  type: num
  description: "Person's age in years"
  required: true

birthday:
  type: date
  description: "Birth date in YYYY-MM-DD format"
  required: false
```

### Available Evaluators

The library automatically selects appropriate evaluators based on data types:

| Type | Evaluator | Description |
|------|-----------|-------------|
| `string` | Text evaluation | Uses LLM-based judgment or n-gram similarity (chrF) |
| `string` (format: date) | Date evaluation | Date format-aware comparison when format is specified |
| `date` | Date evaluation | Date format-aware comparison |
| `integer`, `float`, `number` | Numeric evaluation | Exact numeric equality comparison |
| `enum` | Enum evaluation | Exact match against predefined choices |
| `array`, `list` | List evaluation | Element-wise comparison with configurable aggregation |

### Examples

See the [examples](examples) directory for more comprehensive usage examples and programmatic API usage.

## Citation

```bibtex
# TBA
```

## See also

* [JuDDGES - Judicial Decision Data Gathering, Encoding, and Sharing](https://github.com/pwr-ai/JuDDGES)
